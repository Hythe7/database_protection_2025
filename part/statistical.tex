\section{Безопасность статистических баз данных}
\subsection{Определение статистической базы данных}
Статистическая база данных — это специализированный тип базы данных, который разработан и оптимизирован для хранения и
управления статистическими данными. Она представляет собой совокупность таблиц, где каждая таблица содержит переменные
(столбцы), представляющие различные характеристики или атрибуты, и наблюдения (строки), представляющие отдельные единицы
данных или объекты. Статистические базы данных часто содержат большие объемы данных, собранных из различных источников,
таких как опросы, эксперименты, наблюдения и административные записи. Такие базы данных могут содержать разнообразные
типы данных, включая числовые, категориальные, временные ряды и многие другие. Они могут быть созданы и использованы для
различных целей, таких как проведение исследований, анализ данных, подготовка отчетов, прогнозирование и принятие
решений.


Статистические базы данных обычно имеют стандартизированную структуру и формат, чтобы обеспечить согласованность и
удобство использования данных для статистического анализа. Они также могут включать механизмы для обновления данных,
контроля качества данных и обеспечения конфиденциальности и безопасности информации.

Статистические базы данных могут быть реализованы с использованием различных технологий, включая реляционные базы
данных, NoSQL базы данных и специализированные системы управления базами данных (СУБД), которые оптимизированы для
работы с большими объемами статистических данных. Они могут быть доступны как локально, так и в облачных средах, что
позволяет исследователям и аналитикам легко получать доступ к данным и проводить анализ в любое время и в любом месте.

\subsection{Классификация угроз и уязвимостей}
Безопасность статистических баз данных (СБД) направлена на предотвращение раскрытия
конфиденциальной информации об отдельных индивидах при публикации совокупных данных.
Угрозы для СБД можно разделить на несколько категорий: (1) \textbf{непосредственное
раскрытие личности} – когда злоумышленник прямо идентифицирует запись в БД (например, если
в ответе на статистический запрос фигурирует уникальное значение, позволяющее вычислить
данные конкретного человека); (2) \textbf{раскрытие по совокупности (косвенное раскрытие)}
– когда личная информация выясняется из комбинации статистических результатов или путём
сопоставления с внешними сведениями; (3) \textbf{раскрытие атрибутов} – когда, хотя
личность не идентифицирована точно, об отдельном респонденте выводятся некоторые
чувствительные сведения.

Классическим примером является атака по разнице запросов: злоумышленник может вычитать
результаты двух схожих агрегатных запросов, отличающихся наличием или отсутствием одного
индивида, и таким образом получить информацию о значении конкретной записи. Данная техника
известна как \textit{атака дифференцирования} или «tracker attack» и была
продемонстрирована ещё в ранних исследованиях по статистическим БД.

Современные угрозы усугубляются наличием больших объёмов данных и вычислительной мощности.
Например, \textbf{атаки реконструкции (reconstruction attacks)} позволяют, используя
множество ответов на агрегатные запросы, постепенно восстановить значительную часть
исходных индивидуальных данных. Теоретически доказано, что если система отвечает на
слишком большое число запросов с высокой точностью (минимальным шумом), злоумышленник
может в полиномиальное время реконструировать значительную долю записей исходной базы
\autocite{differentialprivacy-org}. В работе Динура–Нисима (2003) показано, что получение
«слишком точных» (хоть и слегка зашумлённых) ответов на множество запросов приводит к
возможности практически полного восстановления первоначальных данных
\autocite{differentialprivacy-org}.

Эта угроза не просто теоретическая: Бюро переписи населения США между переписями 2010 и
2020 гг. провело внутренний эксперимент по реконструкции, сумев на основе опубликованных
таблиц переписи 2010~г. восстановить значительную часть индивидуальных записей и
сопоставить их с реальными людьми \autocite{cornell-edu}. Таким образом было подтверждено,
что агрегированные статистические данные могут служить «замком, который можно вскрыть»,
если подобрать правильные инструменты \autocite{cornell-edu}.

Данные риски вынудили пересмотреть подходы к конфиденциальности и стали стимулом для
внедрения новых методов защиты, таких как дифференциальная приватность (см. ниже)
\autocite{cornell-edu}.

Ещё одна серьезная угроза – \textbf{атаки на основе внешних данных (linkage attacks)}.
Даже если публикуются лишь обезличенные записи или агрегаты, злоумышленник может
сопоставить их с доступными сторонними источниками. Классический пример – выявление
медицинской записи губернатора Массачусетса У.Уэлда по анонимизированным данным больницы,
которые были связаны с избирательным реестром по дате рождения, полу и почтовому индексу.

В общем случае было показано, что для повторной идентификации индивида в анонимном наборе
данных зачастую достаточно всего \textbf{трёх характеристик: даты рождения, пола и места
жительства (почтового индекса)}. Эти сведения часто присутствуют в открытых источниках и,
комбинируясь, позволяют сопоставить записи с конкретными людьми. Именно поэтому простая
деидентификация (удаление имён, номеров и т.п.) без изменения или обобщения квази-
идентификаторов недостаточна для гарантированной анонимности.

Современные исследования подтверждают, что даже тщательно обезличенные наборы данных могут
сохранять ненулевой риск реидентификации. Так, в одном исследовании, анализировавшем
обезличенный набор данных о больных COVID-19, было показано наличие риска раскрытия
личности во всех рассмотренных сценариях, причём в некоторых случаях злоумышленнику не
требовалось больших усилий для идентификации отдельных людей \autocite{journals-plos-org}.

Наконец, \textbf{угрозы в моделях машинного обучения} становятся актуальными при
использовании статистических данных для построения моделей. К ним относятся атаки по
принадлежности (membership inference), когда атакующий, имея доступ к модели или её
выводам, пытается определить, использовалась ли запись конкретного человека в обучении.
Схожие по сути с дифференцированными атаками на агрегаты, такие атаки могут нарушать
приватность участников обучающих выборок. Ещё одна опасность – утечка информации через
параметры обученной модели (model inversion), когда злоумышленник по доступу к модели
восстанавливает приближённые входные данные.

Эти угрозы подразумевают, что защиту необходимо обеспечивать не только на уровне ответов
на запросы, но и при публикации производных артефактов – обученных моделей, синтетических
данных и т.п.

\subsection{Методы защиты и ограничения доступа}
За десятилетия исследований разработан ряд методов, снижающих риск утечки информации из статистических БД. Их можно
условно разделить на две группы: (1) методы, ограничивающие или контролирующие выдачу результатов запросов, и (2)
методы, искажающие данные или ответы с целью скрыть влияние отдельных записей. На практике часто применяется комбинация
подходов. Ниже рассмотрены классические и современные методы защиты, а также их развитие в свете новейших исследований.

\subsubsection{Ограничение запросов и доступов}

\textbf{Ограничение запросов (query restriction)} – один из первых подходов к безопасности
СБД. Идея в том, чтобы не отвечать на «опасные» запросы, результаты которых могут привести
к раскрытию индивидуальных данных. Реализации этого подхода включают: \textit{Контроль
размеров выборки ответа.} Система отказывает в предоставлении статистики, если запрос
охватывает слишком малое число записей. Например, не выдавать агрегаты по группе из менее
чем $k$ человек (часто $k=3$ или $5$) во избежание раскрытия отдельных участников выборки.
Этот метод давно используется в официальной статистике при публикации таблиц: ячейки,
основанные на малом числе наблюдений, подлежат подавлению (скрытию). \textit{Ограничение
количества запросов или их логики.} Пользователю может быть запрещено делать
неограниченное число произвольных запросов. Например, система может контролировать
перекрытие наборов данных, по которым делаются запросы, не допуская последовательности
запросов, позволяющей вычислить значение отдельной записи. Также возможен мониторинг
(аудит) запросов: если совокупность нескольких запросов в комбинации может вывести
конфиденциальные данные, система блокирует выполнение очередного запроса или добавляет
ограничения. \textit{Упрощение запросов.} В некоторых системах статистический функционал
нарочно ограничен преднабором безопасных операций. Вместо произвольных SQL-запросов
пользователям могут быть доступны только агрегаты по заранее определённым категориям, что
сужает пространство потенциально опасных вопросов. Методы ограничения доступа хороши тем,
что \textbf{не искажают данные}: ответы остаются точными, а риск утечки снижается за счёт
строгих правил доступа.

Однако у этих методов есть серьёзные недостатки. Во-первых, они снижают точность и
гибкость аналитики – многие полезные запросы могут блокироваться системой, что уменьшает
ценность данных для исследователей. Во-вторых, адекватно предусмотреть все опасные
комбинации запросов чрезвычайно сложно; история знает случаи, когда находчивые
пользователи составляли серии «невинных» запросов, которые в совокупности обходили
ограничения. Аудит запросов тоже не гарантирует полного предотвращения утечек и обычно
сложен в реализации. Тем не менее, ограничения по минимальному размеру группы до сих пор
повсеместно используются в статистической практике как базовая мера безопасности
(например, ячейки статистических таблиц с малым числом наблюдений традиционно скрываются
или агрегируются).

\subsubsection{Пертурбация данных: шум, агрегирование и обобщение}

Другой большой класс методов – \textbf{искажение данных или ответов} с целью замаскировать вклад отдельных записей. К
ним относятся добавление случайного шума, агрегация, обобщение, а также методы анонимизации. \textit{Добавление шума к
результатам запросов.} В ранних работах по СБД предлагалось отвечать на каждый запрос слегка изменённым значением –
например, прибавляя случайное число из заданного диапазона или распределения. Таким образом, точные значения скрываются,
но статистика в целом остается приближенно верной. Однако, как впоследствии выяснилось, неконтролируемое добавление шума
может быть недостаточно: злоумышленник, усредняя многократные запросы или комбинируя результаты, способен уменьшить
эффект шума. Если шум мал, возникает риск реконструкции \autocite{differentialprivacy-org} ; если же шум велик, данные
теряют ценность.

Современное развитие этой идеи – \textbf{дифференциальная приватность}, в рамках которой шум
подбирается математически обоснованным образом. \textit{Обобщение и супрессия (анонимизация
данных).} При публикации микро-данных (индивидуальных записей) классическим подходом является $k$-анонимизация. Данные
преобразуются так, что каждая запись «сливается» как минимум с $k-1$ другими записями по ключевым квази-идентификаторам.
Например, даты рождения могут быть заменены на возрастные диапазоны, конкретные адреса – на населённые пункты, и т.д.,
чтобы каждая комбинация таких атрибутов встречалась не меньше $k$ раз в наборе. Дополнительно из данных могут удаляться
(супрессироваться) некоторые поля. Модели $l$-разнообразия и $t$-близости развивают эту идею, требуя разнообразия и
сходства распределения чувствительных атрибутов внутри анонимизованных групп, чтобы затруднить выводы о конкретной
записи. Эти методы реализованы в ряде инструментов и стандартизованы (например, ISO/IEC 20889:2018 описывает основные
подходы деидентификации и их характеристики \autocite{ISO-IEC-20889-2018} ). Преимущество анонимизации – данные остаются
на уровне отдельных записей и могут быть повторно проанализированы различными способами (в отличие от агрегированных
таблиц). Однако серьезный недостаток – уязвимость к атакам связывания: как обсуждалось выше, если злоумышленник обладает
внешними сведениями о конкретных лицах (например, знает их возраст, пол и город), он может найти совпадающие записи в
обезличенном датасете и тем самым деанонимизировать их. Практика и исследования показали, что $k$-анонимность и её
производные не гарантируют приватности в условиях наличия вспомогательной информации. Эксперты отмечают, что
традиционные методы обезличивания (удаление или маскирование полей, укрупнение данных) без дополнительных мер уже «не
работают» против современных методов нападения \autocite{cornell-edu}.

Таким образом, хотя обобщение и супрессия всё ещё
полезны и входят в стандарты защиты данных, полагаться исключительно на них опасно. \textit{Перестановка и обфускация
данных.} В статистической практике применяются также методы, при которых выпущенные микро-данные не соответствуют
реальным единицам, но сохраняют статистические свойства. Например, \textbf{перестановка (swapping)} – когда значения
некоторых атрибутов обменяются между записями случайным образом, так что каждое отдельное лицо «ломается», но
распределения по атрибутам остаются неизменными. Другие методы включают \textbf{микроагрегацию}, когда несколько близких
записей по значениям агрегируются (усредняются) и вместо них публикуется усреднённая запись; \textbf{добавление
«фиктивных» записей} (noise injection) – введение в данные искусственных записей с целью сбить точность реконструкции;
\textbf{округление} значений в опубликованных таблицах до определённого шага и т.д. Эти методы сложнее для атаки, чем
чисто обобщённые данные, однако и они уязвимы при наличии достаточной внешней информации или при применении умных
алгоритмов – современный злоумышленник с помощью перебора и сравнения с публичными данными всё равно может во многих
случаях разгадать «пазл» настоящих значений.

\subsubsection{Дифференциальная приватность}

\label{sec:dp} Одним из наиболее значимых достижений в области формальных гарантий конфиденциальности стал механизм
\textbf{дифференциальной приватности (DP)}. Дифференциальная приватность формально ограничивает, насколько присутствие
или отсутствие отдельной записи может повлиять на результаты статистических запросов. Иными словами, из ответов системы
практически невозможно достоверно определить, участвует ли конкретный индивид в базе данных или нет. Формально условие
$(\varepsilon,\delta)$-дифференциальной приватности требует, чтобы для любых двух соседних наборов данных (отличающихся
наличием одной записи) распределения вероятностей ответов на любые запросы были почти неотличимы (параметры
$\varepsilon$ и $\delta$ задают верхние границы различия).

В прикладном плане дифференциальная приватность достигается
добавлением специально рассчитанного статистического шума или случайным изменением ответа. Например, при агрегировании
числа объектов в выборке к результату прибавляется случайная величина из распределения (как правило, лапласовского или
гауссовского) с параметрами, зависящими от требуемого уровня приватности $\varepsilon$. Чем меньший $\varepsilon$
(строже защита), тем больший шум добавляется. В официальном пресс-релизе, посвящённом переписи 2020 года, Бюро переписи
даёт понятное описание: дифференциальная приватность – это \textit{«математический метод, который добавляет тщательно
откалиброванный статистический шум к набору данных и обеспечивает баланс между приватностью и точностью»}
\autocite{census-gov}. Иначе говоря, вместо публикации точных статистик публикуются слегка искаженные (но в среднем без
смещения) статистики, достаточные для анализа общих тенденций, но не позволяющие уверенно судить об отдельных
участниках. На больших массивах данных влияние шума невелико, а вот для небольших групп (малых поселений, редких
категорий) шум может составлять заметную долю значения, что требует аккуратности при использовании DP на практике
\autocite{cornell-edu}. Важно, что дифференциальная приватность предоставляет \textbf{строгую математическую гарантию}
против широкого класса атак, включая перечисленные выше атаки реконструкции и принадлежности. Если механизм
удовлетворяет DP, никакой злоумышленник, сколь бы мощные вычисления и внешние данные у него ни были, не сможет с высокой
уверенностью вычленить информацию о конкретной записи. Именно по этой причине данная модель защиты сейчас считается
«золотым стандартом» для публикуемых статистических данных. Как отмечают исследователи, в плане абсолютных гарантий
индивидуальной защиты \textit{«дифференциальная приватность – единственный вариант»} на сегодняшний день
\autocite{cornell-edu}.

После введения концепции DP (Дуорк, Кент и др., 2006) прошло почти два десятилетия, и за это
время метод вышел из академических кругов в практику. Крупные технологические компании (Google, Apple, Microsoft и др.)
начали применять дифференциальную приватность при сборе телеметрии и публикации статистики о пользователях. В 2020 году
Бюро переписи США впервые в истории применило дифференциальную приватность для публикации результатов переписи
населения, признав, что прежние методы обеспечения конфиденциальности (супрессия, «случайное округление» и пр.) более не
гарантируют защиту \autocite{cornell-edu}. В опубликованных данных переписи 2020 реализован механизм DP (под названием
\textit{Disclosure Avoidance System}), добавляющий шум в таблицы с учётом глобального бюджета приватности. Как
сообщается, это решение призвано обеспечить баланс между точностью данных и защитой респондентов, и стало ответом на
выявленные угрозы реконструкции в переписи 2010 \autocite{census-gov} \autocite{cornell-edu}.

Дифференциальная приватность применяется как в интерактивных системах (отвечающих на запросы), так и для разовой публикации наборов
данных. В интерактивной среде обычно устанавливается общий «бюджет приватности» $\varepsilon$ на пользователя или на всю
систему, который расходуется с каждым запросом – после исчерпания бюджета дальнейшие запросы могут блокироваться. При
разовой публикации (например, статистического отчёта) методология DP часто используется для генерации
\textbf{синтетических данных} – искусственно сгенерированного датасета, статистически похожего на настоящий, но
удовлетворяющего DP, либо для прямого добавления шума в публикуемые таблицы. DP не лишена недостатков. Главный из них –
\textbf{компромисс между точностью и приватностью}: при сильной защите добавляемый шум может существенно исказить
полезную информацию. Поэтому на практике приходится выбирать $\varepsilon$ достаточно осторожно, иногда допуская
небольшое нарушение строгой DP (параметр $\delta>0$) для улучшения качества. Кроме того, реализация DP требует высокого
профессионализма: неправильное применение механизма или игнорирование некоторых «подводных камней» (например, корреляции
запросов, вычисления на подгруппах и т.д.) может нарушить гарантии. Для помощи практикам Национальный институт
стандартов и технологий США (NIST) выпустил подробные рекомендации по оценке и реализации дифференциальной приватности
\autocite{nvlpubs-nist-gov}, где описывается т. н. «пирамидa дифференциальной приватности» – многоуровневая модель
факторов, влияющих на безопасность DP-систем, и типичные ошибки (hazards) при внедрении DP на практике
\autocite{nvlpubs-nist-gov}.

\subsubsection{Приватность на основе обучаемых моделей (PATE и др.)}

Класс дифференциально-приватных методов был также адаптирован для машинного обучения. Помимо прямого применения DP при
обучении моделей (например, алгоритм DP-SGD – стохастического градиентного спуска с добавлением шума, позволяющий
обучить модель, удовлетворяющую DP требованиям), существуют интересные подходы, использующие ансамбли моделей. Одним из
них является \textbf{PATE (Private Aggregation of Teacher Ensembles)} – приватное агрегирование ансамбля учителей.

В схеме PATE исходные обучающие данные делятся на несколько непересекающихся частей, на каждой обучается своя
модель-«учитель». Затем на входе нового (нечувствительного) набора данных несколько таких моделей-учителей голосуют по
меткам для каждого примера, и эти метки с добавлением небольшого случайного шума используются для обучения финальной
модели-«студента». Поскольку каждый отдельный «учитель» видит лишь часть данных, а до студента доходит только
\textit{защищённый агрегированный} ответ (результат голосования с шумом), гарантируется, что финальная модель не выдаёт
информацию о каких-либо отдельных записях из исходного чувствительного набора. Метод PATE позволяет достичь строгой
дифференциальной приватности для модели-студента, сохраняя при этом высокое качество обучения, особенно при наличии
дополнительной публичной выборки для предобучения.

Помимо PATE, развивается ряд подходов для \textbf{приватного
машинного обучения}: это и упомянутый DP-SGD, и специальные архитектуры, устойчивые к утечке (например, автоэнкодеры,
обученные подавлять личные атрибуты). Цель всех этих методов – обеспечить, чтобы модели, тренируемые на конфиденциальных
данных, не хранили и не разглашали сведения, привязанные к конкретным лицам. В некотором смысле эти методы расширяют
идею статистической базы данных: вместо того чтобы отвечать на отдельные запросы, система обучает общую модель, которая
позволяет делать предсказания или анализ без доступа к сырым данным. Однако без специальных мер такая модель может
оказаться источником утечки, поэтому внедряются механизмы вроде PATE или DP-SGD. Современные исследования в этой области
показывают, что комбинация методов – например, обучение с DP-SGD и последующий анализ модели на предмет утечек
(membership inference tests) – необходима для реальной гарантии приватности. Некоторые конкурсы (как \textit{Netflix
Prize}) и инциденты продемонстрировали, что даже модели или анонимизированные рейтинги могут поддаваться деанонимизации
при сочетании с внешними данными, если не были использованы формальные методы защиты.

Таким образом, приватность в ML –
неотъемлемая часть проблемы безопасности статистических данных, и новейшие методы вроде PATE представляют перспективное
направление защиты.

\subsubsection{Безопасное объединение данных (secure data linkage)}

Во многих случаях ценность представляет объединение данных из разных источников для получения более полной
статистической картины. \textbf{Безопасное объединение (linkage)} – это процедура сопоставления записей, относящихся к
одному и тому же объекту (человеку, организации) из разных баз данных, без раскрытия лишней информации. Классический
подход – через общие идентификаторы (например, если у всех источников есть единый персональный ID). Однако, когда такие
идентификаторы отсутствуют или не могут быть раскрыты из-за конфиденциальности, применяют специальные методы. Одним из
них является \textbf{приватное совпадение множест (Private Set Intersection, PSI)} и вариации.

Стороны, владеющие своими списками записей, могут с помощью криптографических протоколов вычислить пересечение (список общих индивидуумов) 
без раскрытия посторонних данных. Существуют реализации PSI на основе гомоморфного шифрования, протоколов типа
Diffie–Hellman и т.д. Они обеспечивают криптографическую стойкость, но могут требовать значительных вычислительных
ресурсов при большом объёме данных. Более лёгкий в реализации, но менее безопасный подход – \textbf{криптографическое
хеширование идентификаторов} для последующего связывания. Например, два ведомства могут договориться хешировать
паспортные номера граждан одним и тем же ключом и сравнить хеши для стыковки записей.

Однако без должных мер (солирование, использование стойких алгоритмов) такой подход уязвим: злоумышленник, завладевший хешами, может 
попытаться их перебрать и восстановить исходные идентификаторы. Популярный метод кодирования персональных данных для linkage –
через \textbf{Bloom-фильтры}: идентификаторы (например, ФИО, дата рождения) превращаются в битовые векторы с помощью
случайных проекций. Это позволяет делать нечёткие сопоставления (учитывать опечатки, разночтения имен). Но исследования
показали, что и Bloom-фильтры без дополнительных мер могут быть атакованы – с помощью криптоанализа и известных словарей
можно восстановить значимые части исходных данных \autocite{journalprivacyconfidentiality-org}. Поэтому последние
разработки рекомендуют либо усложнять кодирование (например, диффузией, многократным солированием битовых блоков), либо
переходить к проверенно безопасным методам на базе шифрования.


В целом, безопасное объединение данных сейчас достигло
состояния, когда для практических задач доступны инструменты, позволяющие проводить слияние без раскрытия персональной
информации. Например, есть решения для \textit{приватного рекорд-линкинга} в здравоохранении, где больницы могут
выявлять общих пациентов для исследований, не раскрывая остальным ни имён, ни иных PII. При этом всегда важно проводить
анализ рисков: любая упрощённая схема сопоставления должна считаться потенциально раскрывающей, пока не доказано
обратное. В некоторых случаях безопаснее доверить сводное объединение независимому доверенному лицу (trusted third
party), который соберёт исходные данные, обезличит их и передаст статистику – такой организационный метод часто
используется, когда прямой приватный линкинг затруднён.

\subsubsection{Криптографические методы и вычисления на зашифрованных данных}

Помимо статистических методов обезличивания, существует направление, опирающееся на криптографию, – обеспечение
конфиденциальности с помощью шифрования во время обработки. \textbf{Гомоморфное шифрование} позволяет производить
вычисления непосредственно над зашифрованными данными, получая зашифрованный же результат, который можно расшифровать
только имея ключ. Теоретически, статистическая база данных могла бы отвечать на запросы, не «видя» самих данных в
открытом виде. Пользователь, получив зашифрованный ответ, расшифровывает его своим ключом.

На практике полность
гомоморфные системы всё ещё крайне медленны для широкого применения, но частично гомоморфные (поддерживающие
ограниченные виды операций) уже применяются в отдельных проектах. Например, протоколы вычисления по данным нескольких
источников (\textbf{Secure Multi-Party Computation, MPC}) позволяют нескольким держателям данных вычислять обобщённые
статистики, не раскрывая друг другу свои данные. Эти методы математически гарантируют безопасность (при условии
корректной реализации и отсутствия компрометации ключей), но технически сложны.

Другой подход – \textbf{безопасные аппаратные среды} (например, Intel SGX, защищённые enclaves), где данные расшифровываются и 
обрабатываются внутри изолированной доверенной среды на уровне процессора. Пользователи получают результаты, а доступ извне к этим
данным во время обработки исключён аппаратно. Этот путь ещё относительно молод, и исследования показывают наличие уязвимостей в
текущих реализациях (например, побочные каналы могут раскрыть данные из enclave).

Тем не менее, в будущем комбинация
криптографии и аппаратных решений может дать возможность безопасно выполнять произвольные статистические запросы без
оглядки на риски приватности, сводя проблему к доверию в алгоритмы, а не к доверию в намерения пользователя.

\subsection{Практические примеры атак и инцидентов}
Рассмотрим несколько реальных случаев, подчеркнувших необходимость совершенствования методов защиты. \textbf{Перепись
населения (США, 2010)} – как упоминалось, послужила примером успешной реконструкции. Злоумышленник (в данном случае сами
статистики, выступавшие в роли «красной команды») взял опубликованные агрегаты переписи (итоги по населению вплоть до
небольших территорий по возрасту, полу, расе и пр.) и с помощью алгоритма подобрал индивидуальные записи, согласующиеся
со всеми таблицами. Затем, сопоставив эти записи с публично доступными именами из избирательных списков, удалось
правильно идентифицировать значительную долю реальных людей \autocite{cornell-edu}.

Это стало тревожным сигналом для
всего статистического сообщества. В результате Бюро переписи не только перешло на дифференциальную приватность в 2020
г., но и активно публиковало исследования по реконструкции, предупреждая, что аналогичные атаки возможны и на другие
выпуски данных (например, на статистику американского сообщества ACS, на экономические переписи и т.д.). Случай переписи
2010 часто приводят как доказательство: даже крупные агрегированные публикации (\textit{на первый взгляд обезличенные})
имеют \textbf{неявные связи}, которые можно эксплуатировать для деанонимизации \autocite{cornell-edu}. \textbf{Данные о
COVID-19.} Пандемия COVID-19 потребовала срочной публикации детальных данных о заболеваемости, госпитализациях, очагах
инфекции. В ряде стран публиковались пофамильные списки заболевших (в обезличенной форме), детальные хронологии их
перемещений и контактов. Это вызывало как этические вопросы, так и практические случаи раскрытия личности. В частности,
в Южной Корее и некоторых других странах Азии подробные данные о маршрутах заражённых лиц приводили к идентификации этих
людей общественностью, что вело к стигматизации. Более тонкий момент – наборы данных по случаям заболевания, содержащие
возраст, пол, район проживания, даты тестов и пр.: даже если имена удалены, комбинация таких полей плюс знание о соседях
и знакомых позволяли вычислить, кто заболел.

В научной работе 2021 г. был проведён анализ обезличенного списка пациентов
с COVID-19 в одной из стран и смоделированы разные варианты наличия фоновых данных у атакующего. Вывод однозначный: во
всех рассмотренных сценариях \textbf{существует реальный риск раскрытия личности}, а для некоторых злоумышленник может
добиться реидентификации «без особых усилий» \autocite{journals-plos-org}. Авторы демонстрируют конкретную атаку,
подтвердившую возможность идентифицировать человека в «обезличенных» данных о заражённых. Этот пример подчёркивает
важность тщательной проверки наборов на риск раскрытия перед их распространением. В ответ на вызовы пандемии появились
инициативы по \textbf{приватному обмену медицинскими данными}. Например, Австралия (штат Новый Южный Уэльс) разработала
инструмент Personal Information Factor (PIF) для автоматической оценки риска реидентификации и подбора мер защиты перед
публикацией данных по COVID-19. Согласно руководителю проекта, простого удаления идентификаторов недостаточно для
обеспечения требуемого уровня приватности, поэтому PIF использует совокупность метрик и операций по деидентификации,
«ориентируясь на перспективы атакующего» для снижения риска.

\textbf{Атаки на анонимизированные наборы данных.}
Известен ряд случаев, когда организации публиковали вроде бы обезличенные данные, а исследователи (или злоумышленники) успешно
восстанавливали личности. Помимо уже упомянутого случая с медицинскими записями и избирательным списком, резонанс
получили: утечка анонимизированных поисковых запросов AOL (2006) – журналисты по набору запросов идентифицировали
нескольких пользователей; атака Нараянан–Шматикова (2008) на анонимизированные данные Netflix о рейтингах фильмов – по
немногочисленным оценкам на другом сайте были выявлены личности некоторых пользователей Netflix; многочисленные примеры
деанонимизации мобильных данных и данных о перемещениях – показано, что траектория человека по времени и пространству
уникальна почти для каждого, и зная несколько точек можно установить кто это. Эти кейсы хоть и выходят за пределы
«статистических БД» в классическом понимании, но суть их та же: недостаточно проанонимизированные данные быстро
становятся добычей техники реидентификации. Они стимулировали разработку как новых методов (включая дифференциальную
приватность), так и новых нормативных требований к публикации данных.

\subsection{Правовые и этические аспекты защиты статистических данных}
Вопросы безопасности статистических баз данных тесно связаны с правовыми нормами и этическими принципами. В различных
юрисдикциях действуют законы и стандарты, обязывающие защищать конфиденциальность данных, получаемых в статистических
целях. Здесь мы рассмотрим некоторые ключевые положения: международные принципы, законодательство РФ, европейское
регулирование (GDPR) и стандарты безопасности информации.

\textbf{Международные принципы официальной статистики.} ООН
установлены Fundamental Principles of Official Statistics, принятые в 1994 г., которыми руководствуются национальные
статистические службы. Принцип 6 прямо гласит: \textit{«Личные данные, собираемые статистическими ведомствами для
подготовки статистической информации, независимо от того, относятся ли они к физическим или юридическим лицам, должны
носить строго конфиденциальный характер и использоваться исключительно для статистических целей»}
\autocite{unstats-un-org}. Этот принцип отражает фундаментальную этическую норму: данные, собранные от респондентов, ни
при каких условиях не должны быть использованы во вред им или раскрыты. На практике это означает, что статистические
органы обязаны внедрять меры, предотвращающие идентификацию респондентов в публикуемых данных. Аналогичные положения
содержатся и в руководствах ОЭСР по статистической деятельности – конфиденциальность респондентов рассматривается как
ключевое условие сохранения доверия общества к официальной статистике.

\textbf{Законодательство Российской Федерации.} В
РФ действует Федеральный закон № 282-ФЗ «Об официальном статистическом учёте и системе государственной статистики»
(2007). Он закрепляет гарантии конфиденциальности первичных статистических данных. В частности, закон требует
\textit{обеспечить конфиденциальность информации ограниченного доступа}: первичные статистические данные, полученные от
респондентов, не подлежат разглашению или распространению и используются только для формирования агрегированной
официальной статистической информации \autocite{rg-ru}. Таким образом, закон прямо запрещает раскрывать индивидуальные
сведения, собранные в ходе статистических наблюдений, и возлагает ответственность за их защиту на органы статистики.
Кроме того, в России действует Федеральный закон № 152-ФЗ «О персональных данных» \autocite{FZ152}, распространяющийся
на любые операции с персональными данными. Когда статистическое ведомство или исследовательские организации обрабатывают
персональные данные, они должны соблюдать требования 152-ФЗ, включая применение мер по обезличиванию при передаче данных
в обезличенном виде. Следует отметить, что 152-ФЗ допускает использование персональных данных в статистических или
научных целях без согласия субъекта при условии обезличивания – это коррелирует с подходом GDPR (рассмотренным ниже).
Также в России принят ГОСТ Р 59407-2021 «Базовая архитектура защиты персональных данных» \autocite{GOST}, описывающий
методы анонимизации, хотя носит рекомендательный характер.

\textbf{Общее регулирование защиты данных (GDPR).}
Генеральный регламент по защите данных ЕС (GDPR) устанавливает строгие требования к обработке персональных данных, в том
числе и в исследовательских, статистических целях. Один из ключевых принципов GDPR – минимизация данных и
\textbf{«privacy by design»} (приватность по умолчанию при проектировании систем). Применительно к статистическим БД это
означает, что системы должны с самого начала предусматривать методы защиты (как технические, так и организационные) для
предотвращения утечек. GDPR различает понятия \textit{псевдонимизации} и \textit{анонимизации}. Анонимизированные
данные, которые необратимо перестали относиться к идентифицируемому лицу, вообще выводятся из-под действия GDPR (Recital
26) – то есть, если данные успешно обезличены, их можно свободно использовать. Однако достигнуть полного обезличивания
сложно, поэтому чаще речь идёт о псевдонимизации – обработке, после которой данные нельзя приписать конкретному субъекту
без дополнительной информации (хранимой раздельно). GDPR (ст.25 \autocite{GDPR25}) требует применять псевдонимизацию как
одну из мер защиты. Статья 89 GDPR \autocite{GDPR89} специально посвящена гарантиям при обработке данных в
статистических или научных целях: она указывает, что такие данные могут обрабатываться и храниться дольше, чем обычно,
при условии применения надлежащих гарантий (включая псевдонимизацию), и если публикация результатов происходит в
обезличенном виде. Иными словами, европейское право поощряет использование данных для общественно полезных целей
(статистика, наука), но обязывает обезопасить индивидов путём деидентификации результатов. Следует также упомянуть, что
нарушение принципов конфиденциальности статистических данных может привести к серьезным юридическим последствиям. В
странах ЕС нарушение GDPR (утечка персональных данных, недостаточные меры защиты) грозит многомилионными штрафами. В РФ
разглашение персональных сведений вопреки закону также влечёт ответственность (вплоть до уголовной, если будет доказано
злоупотребление). Поэтому правовые нормы стимулируют организации внедрять лучшие технические решения – будь то
дифференциальная приватность, строгая анонимизация или ограничение доступа – чтобы исключить раскрытие личности.

\textbf{Стандарты информационной безопасности.} Помимо специализированных руководств по приватности, существуют общие
стандарты, регламентирующие защиту информации, применимые и к статистическим данным. Международный стандарт
ISO/IEC 27001 устанавливает требования к системе менеджмента информационной безопасности (СМИБ). Организации,
обрабатывающие конфиденциальные данные (включая статистические службы), часто сертифицируют свои процессы по ISO 27001,
что подразумевает проведение анализа рисков, внедрение контролей доступа, шифрование хранения и передачи данных,
обучение персонала и др. Хотя ISO 27001 не диктует конкретных методов анонимизации, он создаёт общий каркас
управляемости безопасностью, внутри которого реализуются и меры по защите данных респондентов. Дополняет его
относительно новый стандарт ISO/IEC 27701 (расширение к 27001, 2019 г.), который фокусируется на управлении
конфиденциальностью персональных данных (PIMS). В нём прописаны процессы и меры, соответствующие требованиям GDPR и
других законов о персональных данных – такие как политика минимизации, управление согласиями, оценка последствий (DPIA).
Статистические ведомства могут внедрять ISO 27701, чтобы структурировать свою работу с данными с учётом приватности по
умолчанию. Уже упомянутый стандарт ISO/IEC 20889:2018 описывает терминологию и классификацию техник деидентификации
данных \autocite{ISO-IEC-20889-2018}, а недавний ISO/IEC 27559:2022 предоставляет рамочный подход (framework) к
обезличиванию данных \autocite{ISO-IEC-27559-2022}. Эти стандарты отражают консенсус экспертов о том, как корректно
применять методы обезличивания и оценивать риски реидентификации. Они полезны тем, что дают организациям единые
ориентиры и понятия (например, разницу между разными формами анонимизации, показателями риска, методами контроля
качества данных после обезличивания). Например, ISO 20889 включает описание $k$-анонимности, $l$-разнообразия,
дифференциальной приватности и других моделей в едином документе, а ISO 27559 расширяет его до некоего «руководства по
лучшим практикам». Таким образом, международные стандарты становятся опорой для выработки внутренних политик
безопасности данных в статистике.

\textbf{Этические аспекты.} Помимо сугубо правовых требований, существует этическое
измерение. Доверие общества к статистике – ключевой ресурс. Если граждане опасаются, что их данные могут быть раскрыты,
они могут отказываться участвовать в переписях или опросах, либо давать неверные сведения. Поэтому статистики
придерживаются принципа \textit{«не навреди респонденту»}: никакие действия не должны приводить к тому, что конкретное
лицо будет опознано и понесёт ущерб (репутационный, финансовый и т.д.) от публикации статистики. Этот принцип иногда
выходит за рамки формальной деидентификации – например, могут не публиковаться данные по очень малым территориям или
редким категориям вовсе, даже если формально прямой иденфикации нет, из осторожности (чтобы избежать косвенной
иденфикации через «догадывание» в локальных сообществах). Этические комитеты при статистических службах оценивают, не
приведёт ли публикация тех или иных данных к нежелательным последствиям. К примеру, во время COVID-19 некоторые власти
решили не публиковать слишком детальную информацию о больных, чтобы не нарушать их право на частную жизнь, хотя
юридически могли бы. Этические соображения включают также \textbf{пропорциональность} – баланс между пользой от открытых
данных и потенциальным вредом от утечки. Например, публикация обезличенных медицинских данных может ускорить
исследования и принести пользу обществу, но риск для частной жизни пациентов должен быть минимизирован. Метод
дифференциальной приватности во многом родился именно как ответ на этический вызов: как измерить, какой «вклад» каждого
индивида и насколько мы рискуем его приватностью ради общей пользы данных. DP позволила это количественно формулировать.
Наконец, этика предусматривает и \textbf{прозрачность методов}: данные должны быть защищены, но и потребители статистики
имеют право понимать, какие искажения внесены. Статистические органы публикуют методологические заметки о применении SDC
(Statistical Disclosure Control) методов, об объёмах добавленного шума, об уровнях агрегирования. Это необходимо, чтобы
сохранить доверие исследователей к данным и позволить им интерпретировать результаты с учётом вмешательства в данные.

\subsection{Сравнительная характеристика методов защиты}
Для наглядности приведём таблицу, сравнивающую основные методы обеспечения конфиденциальности в статистических базах
данных по их достоинствам, недостаткам и области применимости.
\begin{table}[H]\small\centering
    \caption{Сравнение методов защиты статистических данных}\label{tab:methods}
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{|p{3.4cm}|p{4.8cm}|p{4.8cm}|p{3.8cm}|}
    \hline
    \textbf{Метод} & \textbf{Достоинства} & \textbf{Недостатки} & \textbf{Применимость} \\ \hline
    
    \textbf{Ограничение запросов и доступа} (контроль размера выборки, аудит запросов) &
    Не искажает данные – точность результатов сохраняется; простота понимания; можно интегрировать в систему прав доступа. &
    Сильно ограничивает гибкость анализа (многие запросы запрещены); трудно учесть сложные комбинации запросов; не защищает при полном доступе к данным. &
    Интерактивные базы данных с агрегированным доступом; используется в официальной статистике для таблиц (порог $k$ для ячеек). \\ \hline
    
    \textbf{Добавление шума к ответам} (простая пертурбация, без формальной модели) &
    Сохраняет суммарные характеристики в среднем; легко реализовать – добавить случайное число; увеличивает неопределённость для атакующего. &
    Может быть нейтрализован усреднением; трудный подбор уровня шума; нет формальных гарантий. &
    Интерактивные системы с большим числом запросов; ранее – для отдельных опубликованных таблиц (например, случайное округление). \\ \hline
    
    \textbf{Агрегирование и супрессия} ($k$-анонимность, обобщение, удаление полей) &
    Простое концептуально; снижает риск прямой идентификации; совместимо с традиционными подходами публикации. &
    Не защищает от связывания с внешними данными; ухудшает детализацию и точность; не имеет количественной оценки риска. &
    Подготовка обезличенных микро-данных для открытой публикации; используется в открытых данных и дата-сетах. \\ \hline
    
    \textbf{Перестановка, обмен, синтетические данные} (маскировка без агрегирования) &
    Сохраняет многие статистические свойства; уменьшает риск связывания напрямую. &
    Может искажать взаимосвязи; сложно проверить, нет ли утечек; нет строгой гарантии анонимности. &
    Публикация для исследовательских целей; используется при запрете на прямой доступ к данным. \\ \hline
    
    \textbf{Дифференциальная приватность (DP)} &
    Строгая математическая гарантия; настраиваемый уровень защиты ($\varepsilon$); композируемость. &
    Добавляет шум; сложность настройки; требует квалификации; трудно объяснить неспециалистам. &
    Интерактивные системы; публикация агрегатов с контролем погрешности; генерация синтетических данных. \\ \hline
    
    \textbf{Обучение приватных моделей (PATE, DP-SGD и др.)} &
    Позволяет использовать данные для ML без раскрытия; может иметь формальную гарантию (DP); снижает риск утечек. &
    Сложность реализации; модели могут быть атакуемы; нужно много данных. &
    Обучение моделей на чувствительных данных (медицина и др.); сценарии, где модель передаётся, а данные нет. \\ \hline
    
    \textbf{Криптографические вычисления (MPC, гомоморфное шифрование)} &
    Максимальная конфиденциальность; нет искажений результата. &
    Очень высокие вычислительные затраты; сложность программирования; не защищает от «слишком точных» запросов. &
    Совместный анализ данных между организациями без их раскрытия; сценарии с высокой ценностью данных. \\ \hline
    
    \end{tabular}%
    }
\end{table} 
В таблице перечислены далеко не все методы, но охватываются основные. Как видно, \textit{универсального решения нет}:
каждый подход имеет свои сильные и слабые стороны. На практике часто применяют
комбинации – например, сначала данные обезличивают (анонимизируют), затем для публикации отдельных показателей поверх
накладывают дифференциально-приватный механизм. Также методы отличаются по применимости: то, что годится для регулярной
официальной статистики (например, подавление малых ячеек и добавление шума в таблицы) может быть неприемлемо для разовой
научной задачи с небольшим набором данных (где предпочтительнее выпустить синтетические данные или дать доступ в
защищенной среде). Поэтому специалисты по защитe информации всегда анализируют контекст, тип данных, угрозы и только
потом выбирают подходящий инструментарий.

\subsection{Инструменты и библиотеки для защиты данных}
В последние годы появилось множество программных средств, облегчающих применение перечисленных методов на практике. Ниже
приведён список некоторых популярных ресурсов и инструментов, используемых для обеспечения конфиденциальности в
статистических данных:\\
\textbf{ARX Data Anonymization Tool} – открытое программное обеспечение для обезличивания данных
(Java-библиотека с графическим интерфейсом). Поддерживает широкий спектр методов анонимизации: $k$-анонимность,
$l$-разнообразие, $t$-близость, микрослияние, удаление идентификаторов и оценку риска. ARX способен обрабатывать наборы
данных большого объёма, предоставляя пользователю отчёты о степени риска раскрытия после применённых трансформаций.
Разработчики отмечают, что ARX – \textit{«комплексное open source решение для анонимизации чувствительных персональных
данных, поддерживающее широкий набор моделей приватности и метрик риска»} \autocite{arx-deidentifier-org}. Инструмент
активно используется в исследовательских проектах и рекомендован в ряде учебных курсов по SDC.\\
\textbf{SdcMicro(R-пакет)} – набор функций на языке R для статистического контроля раскрытия. Предназначен для анонимизации микро-данных
и оценки риска раскрытия. Включает реализации таких методов, как: глобальная и локальная супрессия, рекодирование
(обобщение) категорий, микросмешивание (microaggregation), добавление шума, пострагрууппировочная сводка данных и т.д.
Также sdcMicro может рассчитывать меры риска: $k$-анонимность, вероятность реидентификации записей, информационные
потери. Согласно документации, \textit{«sdcMicro – свободный пакет с графическим интерфейсом, позволяющий генерировать
защищенные микро-данные для исследователей и публики; он включает разнообразные методы снижения риска раскрытия и оценки
информационных потерь»}. Этот инструмент широко применяется статистическими агентствами (например, для подготовки
научно-использовательских файлов данных).\\
\textbf{Google Differential Privacy Library (и обёртки PyDP/PipelineDP)} –
открытая библиотека дифференциальной приватности от Google, написанная на C++ с привязками к Java и Python. В 2019 году
Google выложила в открытый доступ свой внутренний инструмент DP, используемый для агрегирования статистики об
использовании приложений. Библиотека позволяет простым образом применять DP-алгоритмы к набору данных или потокам логов:
содержит реализации механизмов Лапласа и Гаусса, вычисление гистограмм, перцентилей, средних с DP-гарантиями. Имеются
утилиты для отслеживания «бюджета приватности».\\
Python-обёртка PyDP и проект \textbf{PipelineDP} (интеграция с Apache Beam) упрощают внедрение DP для разработчиков на высокоуровневых языках. Как отмечается в документации, \textit{«в 2019 году Google открыла исходный код своей библиотеки дифференциальной приватности для всеобщего использования»}
\autocite{pydp-readthedocs-io}. Сейчас этой библиотекой пользуются и сторонние компании, и исследователи для своих
проектов – она стала своеобразным стандартом де-факто.
\textbf{OpenDP} – это инициатива Гарварда и Microsoft по созданиюоткрытой экосистемы инструментов дифференциальной приватности. В рамках OpenDP развивается библиотека \textbf{SmartNoise} (ранее PSI), предоставляющая средства для применения DP к данным (особенно фокус – SQL-подобные запросы к базам с DP). Также OpenDP занимается проверкой и верификацией DP-реализаций, создает учебные материалы. Этот
ресурс полезен тем, кто хочет внедрять DP с уверенностью в корректности: код проходит ревью сообщества экспертов.\\
\textbf{SDC Toolkit и другие утилиты} – существует набор небольших утилит, например, $\mu$-Argus и $\tau$-Argus
(исторические инструменты для защиты табличных данных и микро-данных, разработанные в Евростате), различные пакеты для
Python (например, \textit{PyAnon}, \textit{dominance analysis} для табличных данных), а также специализированные
продукты. Некоторые коммерческие решения предлагают «конфиденциальность как услугу», внедряя DP для клиентов (например,
система от компании Tumult Labs для защищённой публикации показателей).\\
\textbf{Библиотеки для безопасного ML:}
\textit{TensorFlow Privacy} (от Google) и \textit{PyTorch Opacus} (от Meta) – надстройки к популярным фреймворкам
глубинного обучения, реализующие приватный SGD и другие техники. Они позволяют обучить нейросеть с гарантией
дифференциальной приватности, задав параметр $\varepsilon$. Также \textit{CrypTen} и \textit{TF Encrypted} – библиотеки
для криптографического secure computation с фокусом на ML (например, приватное предсказание: модель и данные
зашифрованы, а вывод получается открытым без раскрытия входа). Эти инструменты пока нишевые, но быстро развиваются,
поскольку спрос на приватный ML растёт.\\
\textbf{Различные оценки и утилиты рисков.}
Помимо инструментов преобразования данных, важны и инструменты \textbf{оценки уязвимостей}. Например, пакет \textit{REXUIs (Re-Identification Risk Utility Interface)} позволяет вычислять вероятность реидентификации для выпущенного набора данных на основе множества сценариев.\\
Платформа \textbf{Amnesia} (от EU data portal) интерактивно ведёт пользователя через процесс анонимизации набора данных
и показывает риск на каждом шаге. Вспомогательные библиотеки вроде \textit{ts-compare} позволяют сравнить две публикации
данных на предмет потенциальной утечки из-за перекрёстной информации (актуально, когда один и тот же массив публикуется
в разное время с разной степенью детализации). Подводя итог: профессиональное сообщество всё более опирается на готовые
инструменты и стандарты в вопросах обеспечения конфиденциальности статистических данных. Это снижает «порог входа» для
организаций, позволяя применять сложные методы (как дифференциальная приватность) без реализации с нуля. Тем не менее,
выбор подходящего инструмента – ответственная задача, требующая понимания как возможностей, так и ограничений каждого.
Например, ARX хорошо подходит для классической анонимизации данных, но не даст формальных гарантий; Google DP library
даст гарантии, но её надо правильно настроить и возможно пожертвовать точностью. Поэтому специалисты зачастую
комбинируют средства: сначала используют ARX/sdcMicro для грубой обезлички, потом применяют DP-библиотеку для финальной
публикации агрегатов. Защита статистических баз данных сегодня – динамично развивающаяся область. С одной стороны, рост
вычислительных возможностей и появление новых атакующих методов (машинное обучение, AI для поиска уязвимостей) ставят
всё новые вызовы. С другой – появляются и более продвинутые средства защиты, опирающиеся на междисциплинарный подход:
математика, информатика, право, этика. Комплексное применение этих средств и следование лучшим мировым практикам (NIST,
ISO, GDPR и т.п.) позволяет максимально снизить риски и обеспечить, чтобы ценная статистическая информация служила
обществу, не нарушая права отдельных граждан.

